{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0fdf9a-e5e1-4929-89cf-597bdcf53fe0",
   "metadata": {},
   "source": [
    "## Data Cleaning & Modelling\n",
    "In this notebook we seek to aggregate the data we've scraped and gathered, clean it, and prepare it for use in our modelling (whether primarily statistical or ML-based). Data has been extracted from multiple sources (primarily wikipedia tables), saved in excel flat files. We then build our recommendation system based on selected specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23df8b6f-efe2-4d37-b550-db38d546bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from itertools import repeat\n",
    "import miceforest as mf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gower\n",
    "import re\n",
    "import os\n",
    "\n",
    "# are we loading up from saved data\n",
    "load_data = True\n",
    "\n",
    "# set params for imputation\n",
    "seed = 100\n",
    "iters = 50\n",
    "datasets = 20\n",
    "kernel_path = \"mice_kernel\"\n",
    "\n",
    "# set path for saving/loading aggregate data\n",
    "aggregate_path = \"Aggregate_Destination_Data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76574fd1-84c0-48b1-85f6-769a0f6132b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously saved kernel and data\n",
    "if load_data:\n",
    "    aggregate_data = pd.read_excel(aggregate_path, index_col=0)\n",
    "    kernel = mf.load_kernel(kernel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c96b3e-3f02-42e0-9deb-38fd0755fcee",
   "metadata": {},
   "source": [
    "### Load Raw Data\n",
    "Here we load raw data from excel files in the designated directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6d2549-01bf-4670-b896-d157545ccd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve master list of destinations\n",
    "destinations = pd.read_excel(\n",
    "    io=\"Travel_Destinations.xlsx\",\n",
    "    header=None\n",
    ")[0].values.tolist()\n",
    "\n",
    "# Retrieve file names and contruct paths for all raw data files\n",
    "dir_name = \"raw_data\"\n",
    "data_files = os.listdir(dir_name)\n",
    "data_paths = list()\n",
    "for file_name in data_files:\n",
    "    data_paths.append((file_name, os.path.join(dir_name, file_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fd64b-8757-4acb-9e9a-b1b1bbecce45",
   "metadata": {},
   "source": [
    "### Fuzzy matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5270c3d-08ec-4989-af4d-50114671e388",
   "metadata": {},
   "source": [
    "The goal here is that for each table in an excel flat file, given an instance in the data which has an associated destination, we want to compare the destination in the data with our master list of destinations and use fuzzy matching to select the most appropriate match. We replace the original destination with the matched destination. Upon completion, each data set will have the same set of destinations listed, making aggregation more accurate and feasible.\n",
    "\n",
    "Fuzzy matching typically uses the Levenshtein distance (aka the edit distance) or Indel distance as a basis. Both compute the number of operations, or edits, required to transform one string into another. The deletion, insertion, or substitution of a character all result in +1 to the Leneshtein distance (all actions are weighted equally). Related, the Indel distance is similar to the Levenshtein distance except it does not permit substitutions. Effectively, any substitution that would have incurred a cost of +1 is replaced by an insertion and deletion with cost +2, or equivalently, a substitution is given a weight of +2. The normalized distance for Levenshtein is computed as $$\\frac{distance}{max(len(string1),\\ len(string2)}$$ whereas the normalized distance for Indel is computed as $$\\frac{distance}{len(string1)\\ +\\ len(string2)}$$\n",
    "\n",
    "The similarity between two strings is $1-distance$. When using a specific ratio in the FuzzyWuzzy package, the function preprocesses the input strings in some way and then computes the scaled Levenshtein similarity. The Token Sort Ratio splits a string into its tokens/words, makes them lowercase and removes punctuations, then sorts the tokens alphabetically and joins them prior to computing the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62150279-55d1-4575-8b98-f67d6a1f5b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 did not match 0 entries\n",
      "Table 2 did not match 0 entries\n",
      "Table 3 did not match 0 entries\n",
      "Table 4 did not match 0 entries\n",
      "Table 5 did not match 1 entries\n",
      "---Vatican City\n",
      "Table 6 did not match 0 entries\n",
      "Table 7 did not match 0 entries\n",
      "Table 8 did not match 0 entries\n",
      "Table 9 did not match 0 entries\n",
      "Table 10 did not match 0 entries\n",
      "Table 11 did not match 0 entries\n",
      "Table 12 did not match 0 entries\n",
      "Table 13 did not match 0 entries\n",
      "Table 14 did not match 0 entries\n",
      "Table 15 did not match 0 entries\n",
      "Table 16 did not match 0 entries\n",
      "Table 17 did not match 0 entries\n",
      "Table 18 did not match 0 entries\n",
      "Table 19 did not match 0 entries\n",
      "Table 20 did not match 0 entries\n",
      "Table 21 did not match 0 entries\n",
      "Table 22 did not match 0 entries\n",
      "Table 23 did not match 0 entries\n",
      "Table 24 did not match 0 entries\n",
      "Table 25 did not match 0 entries\n",
      "Table 26 did not match 0 entries\n"
     ]
    }
   ],
   "source": [
    "fuzzy_dataframes = list()\n",
    "fuzzy_path = \"fuzzy_data\"\n",
    "if not os.path.exists(fuzzy_path):\n",
    "    os.mkdir(fuzzy_path)\n",
    "\n",
    "# for every file and table within\n",
    "table_count = 0\n",
    "for data_path in data_paths:\n",
    "    file_name = data_path[0]\n",
    "    file_path = data_path[1]\n",
    "    table_count += 1\n",
    "    \n",
    "    # open the file\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # read the file and extract its list of destinations\n",
    "        unmatched_data = pd.read_excel(f, index_col=0)\n",
    "        unmatched_destinations = unmatched_data.index.values.tolist()\n",
    "        \n",
    "        # perform fuzzy match \n",
    "        matched_destinations = list(map(\n",
    "            process.extractOne,  # function to be applied\n",
    "            unmatched_destinations,  # applied to \n",
    "            repeat(destinations),  # compared against\n",
    "            repeat(process.default_processor),  # use default processor each time\n",
    "            repeat(fuzz.token_sort_ratio),  # use token sort ratio instead of default\n",
    "            repeat(75)  # similarity must be higher than 50% to be considered a match\n",
    "        ))\n",
    "\n",
    "        # debugging the fuzzy match logic\n",
    "        unmatched_count = matched_destinations.count(None)\n",
    "        print(f\"Table {table_count} did not match {unmatched_count} entries\")\n",
    "        if unmatched_count > 0:\n",
    "            unmatched_indices = [i for i, match in enumerate(matched_destinations) if match == None]\n",
    "        \n",
    "        # check if any entries are None indicating no match, if so, set to previous value \n",
    "        matched_destinations = [unmatched_destinations[index] if match == None else match[0] for index, match in enumerate(matched_destinations)]\n",
    "\n",
    "        # debugging the fuzzy match logic\n",
    "        if unmatched_count > 0:\n",
    "            for i in range(unmatched_count):\n",
    "                print(f\"---{matched_destinations[unmatched_indices[i]]}\")\n",
    "                destinations.append(matched_destinations[unmatched_indices[i]])  # add legitimate unmatched destinations to main list\n",
    "        \n",
    "        # reindex the data to the fuzzy matched destinations\n",
    "        matched_data = unmatched_data.set_index(pd.Index(matched_destinations))\n",
    "\n",
    "        # save the file name in the fuzzy folder\n",
    "        matched_data.to_excel(os.path.join(fuzzy_path, file_name))\n",
    "        fuzzy_dataframes.append(matched_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b22c72-bc5c-4200-981a-0650c2a855d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer join all data based on the destinations\n",
    "aggregate_data = fuzzy_dataframes[2].join(\n",
    "    fuzzy_dataframes[0:2]+fuzzy_dataframes[3:],\n",
    "    how=\"outer\",\n",
    "    sort=True\n",
    ")\n",
    "\n",
    "# remove duplicate rows that end up in the data\n",
    "aggregate_data = aggregate_data[~aggregate_data.index.duplicated(keep='first')]\n",
    "\n",
    "# drop Vatican City\n",
    "aggregate_data.drop(labels=\"Vatican City\", axis=0, inplace=True)\n",
    "\n",
    "# convert object columns to categories\n",
    "for col in aggregate_data.dtypes[aggregate_data.dtypes == aggregate_data.dtypes[0]].index:\n",
    "    aggregate_data[col] = aggregate_data[col].astype(\"category\")\n",
    "\n",
    "# get rid of all weird characters from column names \n",
    "aggregate_data = aggregate_data.rename(columns = lambda x: x.replace('â€“', '-'))\n",
    "aggregate_data = aggregate_data.rename(columns = lambda x: re.sub('[^A-Za-z0-9_%()$./<-]+', '', x))\n",
    "\n",
    "# save aggregate data\n",
    "aggregate_data.to_excel(aggregate_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394c425-8508-487a-9e8f-553a85c0bbcd",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae5961-5478-4a4d-9f98-e9dfca74d681",
   "metadata": {},
   "source": [
    "Here we perform imputation to fill in missing values. There are no destinations in this data that have all missing values. We do not drop any destinations from the analysis as we prefer to have a broader set to recommend to end-users.\n",
    "\n",
    "In this analysis we use multiple imputation by chained equations (MICE) as our imputation method, due to its attempts to account for the uncertainty in the imputed value by provingd both within-imputation and between-imputation variability, a characteristic which makes multiple imputation preferable over single imputation in most circumstances.\n",
    "\n",
    "There continues to be discussion regarding the order of operations between certain data preprocessing steps, namely scaling data via standardization/normalization and imputation. Which of the two procedures comes first may largely depend on the intent of the analysis, imputation method, and modelling that is done downstream. MICE is not necessarily sensitive to scale and with the use of predictive mean matching we are able to avoid imputing nonsensical values (e.g., negative population for a destination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee6a53d-dd77-4cdb-9182-418e12899091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kernel for multiple imputation\n",
    "kernel = mf.ImputationKernel(\n",
    "    data = aggregate_data,\n",
    "    datasets = datasets,\n",
    "    train_nonmissing=False,\n",
    "    mean_match_scheme=mf.mean_match_shap,\n",
    "    save_all_iterations=True,\n",
    "    save_models=1,\n",
    "    copy_data=True,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# iteratively impute data and save final kernel\n",
    "kernel.mice(iters)\n",
    "kernel.save_kernel(kernel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d50cd1-b7aa-4a87-83e6-59cf644ac658",
   "metadata": {},
   "source": [
    "### Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fabb9-e207-4e50-83fc-e28bf8130045",
   "metadata": {},
   "source": [
    "Recommender systems come in various shapes. \n",
    "\n",
    "Some necessitate that you have information about users and their feelings towards the items being recommended, typically collected in the form of user provided ratings. This is typical of approaches based on `collaborative filtering`. From there, memory-based or model-based approaches are specifically taken, but in general, the focus of these methods are in recommending items to users based on the user's similarity to other users, inferring that if users are similar they are likely to like items that other similar users liked. \n",
    "\n",
    "There are also `content based systems` which usually do not have as much data on ratings but supplement this with data related to the attributes of the items being recommended. These systems generally try to recommend items similar to those a user has liked before based on their ratings and the attributes of the item. Hybrid systems also exist, which are likely much more robust in practice.\n",
    "\n",
    "Here, given the raw data and the lack of user ratings on destinations we suffer from a sort of cold-start problem - how do we recommend new destinations to a user if we have no information on how users feel about the destinations we could possibly recommend? As a result, we take a more simplistic approach similar to content based systems. We use distance and similarity measures to determine what to recommend to a user based on their own past travel experiences. Specifically we use Gower's distance which can handle both categorical and continuous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d438d5bb-e9c3-4bf1-b6d7-34bc43c3acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define travel recommendation function\n",
    "def get_recommendations(kernel, data_sets, user_destinations, similar=\"Y\", recs=5):\n",
    "    \"\"\"\n",
    "    A function which generates recommendations for travel destinations based on user input\n",
    "    using Gower's distance and voting ensemble principles for aggregating results across\n",
    "    imputed data sets.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel (miceforest.ImputationKernel) : kernel dataset created by the miceforest package \n",
    "    data_sets (int) : count of data sets created through MICE and stored in kernel\n",
    "    user_destinations (np.ndarray) : array of user provided destinations\n",
    "    similar (str) : 'Y' or 'N' indicating user wants similar or dissimilar recommendations\n",
    "    recs (int) : number of recommendations wanted by the user\n",
    "    \"\"\"\n",
    "    \n",
    "    num_destinations = len(user_destinations)\n",
    "    recommendations = np.array(list())\n",
    "\n",
    "    for data in range(data_sets):\n",
    "        imputed_df = kernel.complete_data(data)\n",
    "        \n",
    "        # get the df indices of the destinations provided by the user\n",
    "        user_dest_idx = np.array([imputed_df.index.get_loc(loc) for loc in user_destinations])\n",
    "        \n",
    "        # get the computed distances for the provided destinations\n",
    "        dist = gower.gower_matrix(np.asarray(imputed_df))[user_dest_idx, :]\n",
    "        \n",
    "        # get the indices of the user provided destinations which should not be recommended again\n",
    "        excl_dest = (dist == 0).sum(axis=0).nonzero()[0]\n",
    "        \n",
    "        # compute the sum of the distances for the set of provided destinations\n",
    "        dist_sum = dist.sum(axis=0)\n",
    "        \n",
    "        if similar == \"Y\":\n",
    "            # retrieve the top n most similar destinations - minimize the sum of distances across the provided locations\n",
    "            temp_recs = np.argpartition(dist.sum(axis=0), (recs+3))[:(recs+3)]\n",
    "        else:\n",
    "            # retrieve the top n least similar destinations - maximize the sum of distances across the provided locations\n",
    "            temp_recs = np.argpartition(dist.sum(axis=0), -(recs+3))[-(recs+3):]\n",
    "        \n",
    "        # exclude any of the user provided destinations\n",
    "        for loc in excl_dest:\n",
    "            temp_recs = temp_recs[temp_recs != loc]\n",
    "        \n",
    "        # provide the final list of recommendations that will be output to the user\n",
    "        final_recs = temp_recs[:recs]\n",
    "        final_recs = np.array(imputed_df.index[final_recs])\n",
    "    \n",
    "        # aggregate recos across data sets\n",
    "        recommendations = np.append(recommendations, final_recs)\n",
    "    \n",
    "    # select the most frequently recommended locations across data sets\n",
    "    recommendations = np.unique(recommendations, return_counts=True)\n",
    "    recommendations = recommendations[0][np.argsort(recommendations[1])[-recs:]]\n",
    "\n",
    "    # return list of top n recommendations based on countries provided and similarity/dissimilarity \n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def print_recommendations(recommendations, similar=\"Y\", recs=5):\n",
    "    \"\"\"\n",
    "    A function which outputs the results provided by recommender system.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    recommendations (np.ndarray) : array of recommended destinations\n",
    "    similar (str) : 'Y' or 'N' indicating user wants similar or dissimilar recommendations\n",
    "    recs (int) : number of recommendations wanted by the user\n",
    "    \"\"\"\n",
    "    \n",
    "    if similar == \"Y\":\n",
    "        similar = \"similarity\"\n",
    "    else:\n",
    "        similar = \"dissimilarity\"\n",
    "    \n",
    "    print(f\"Thanks for your patience, here are {recs} recommendations based on {similar}:\")\n",
    "    for i, loc in enumerate(recommendations):\n",
    "        print(f\"{i+1}. {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe3a72d-36d1-4bf7-9f67-27ef79d38436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example user input\n",
    "user_destinations = np.array([\"Cuba\", \"Mexico\"])  # user provided destinations\n",
    "num_destinations = len(user_destinations)\n",
    "similar = \"Y\"  # user desires similar recommendations\n",
    "recs = 5  # number of recommendations desired by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1447ee72-6dc4-4485-bfee-82f460aba99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for your patience, here are 5 recommendations based on similarity:\n",
      "1. Colombia\n",
      "2. El Salvador\n",
      "3. Bolivia\n",
      "4. Dominican Republic\n",
      "5. Venezuela\n"
     ]
    }
   ],
   "source": [
    "recommendations = get_recommendations(\n",
    "    kernel,\n",
    "    datasets,\n",
    "    user_destinations,\n",
    "    similar, \n",
    "    recs\n",
    ") \n",
    "\n",
    "print_recommendations(recommendations, similar, recs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
