{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478c76b3-94ce-466b-b551-53a963dc22a6",
   "metadata": {},
   "source": [
    "# Webscraping Wikipedia\n",
    "Data about countries globally is not typically available in nice \"digestible\" and downloadable formats, and additionally, tends to be disproportionately available for countries considered \"developed\". We would like to make a broader set of recommendations available to the user by leveraging data related to a diverse set of countries and their attributes, ranging from geography and climate to demographics and health. With that in mind we use Wikipedia, which acts as a central data aggregator, as our data source in this work. \n",
    "\n",
    "It is important to note that given the open and editable nature of Wikipedia pages, there are bound to be inaccuracies. That said, for the purposes of this tool, we are willing to accept and tradeoff the data quality in exchange for the breadth of data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b030e11-c409-4592-a783-9628d51864ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e4501-a2cf-4a34-a52a-ab60671e9b20",
   "metadata": {},
   "source": [
    "Below you can find a helper Class for scraping data from tables embedded in Wikipedia webpages. Ultimately, Wikipedia tables can be quite non-standard with respect to formatting, html tags, and headers used, so you may not be able to use this Class to automate your entire webscraping pipeline. Nonetheless, it acts to speed up the data procurement process across many Wikipedia pages that share similar themes in formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83cdddec-0fb1-400e-896f-d891c614e9fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WikiTable:\n",
    "    \"\"\"\n",
    "    A class that assists with the scraping and extraction of data from tables embedded within Wikipedia pages.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url (str) : a valid url for the target wikiepedia page\n",
    "    category (str): a descriptive category that helps group and classify the type of data being mined\n",
    "    table_num (int) : a number identifying the particular data table to scrape which should be >= 1, with 1 indicating the first table encountered when scrolling on the wiki page\n",
    "    \"\"\"\n",
    "    def __init__(self, url, category, table_num):\n",
    "        # Set url and target table we're interested in\n",
    "        self.wiki_url = url\n",
    "        self.wiki_category = category\n",
    "        self.table_num = table_num\n",
    "        self.data_notes = \"\"\n",
    "\n",
    "        # Open wiki page, retrieve HTMl\n",
    "        page_bytes = urlopen(self.wiki_url).read()\n",
    "        page_content = page_bytes.decode(\"utf-8\")\n",
    "        page_html = BeautifulSoup(page_content)\n",
    "\n",
    "        # Extract page title and target table\n",
    "        self.wiki_title = page_html.h1.text\n",
    "        self.wiki_table = page_html.find_all(\"tbody\")[self.table_num - 1]\n",
    "        self.table_data = None\n",
    "\n",
    "    # print string repr of WikiTable object\n",
    "    def __str__(self):\n",
    "        return f\"Table {self.table_num} in the webpage titled: {self.wiki_title}\"\n",
    "\n",
    "    # get wikipedia page url\n",
    "    def get_url(self):\n",
    "        return self.wiki_url\n",
    "\n",
    "    # get the associated category\n",
    "    def get_category(self):\n",
    "        return self.wiki_category\n",
    "\n",
    "    # set any special notes for this data\n",
    "    def set_notes(self, text):\n",
    "        self.data_notes = text\n",
    "\n",
    "    # show special notes for this data\n",
    "    def show_notes(self):\n",
    "        if self.data_notes != \"\":\n",
    "            print(self.data_notes)\n",
    "        else:\n",
    "            print(\"Notes on data cleaning and treatment are empty\")\n",
    "\n",
    "    # get a copy of the data that was scraped\n",
    "    def get_table(self):\n",
    "        return self.table_data.copy()\n",
    "\n",
    "    def __extract_col_data(self, col_item):\n",
    "        return col_item.get_text().rstrip()\n",
    "\n",
    "    def __extract_row_data(self, row_item, country_index):\n",
    "        if \"<td\" in str(row_item):\n",
    "            col_items = row_item.find_all(\"td\")\n",
    "            try:\n",
    "                col_items[country_index-1] = col_items[country_index-1].find_all(\"a\")[0]\n",
    "            except:\n",
    "                print(\"Operation unsuccessful, no <a> tag, likely a non-country encountered\")\n",
    "        else:\n",
    "            col_items = row_item.find_all(\"th\")\n",
    "        return list(map(self.__extract_col_data, col_items))   \n",
    "\n",
    "    # extract data from the wikipedia table html and create a pandas dataframe \n",
    "    # country_index is the column in which the countries are contained in the table (indexed at 1)\n",
    "    def create_table(self, get_data=False, country_index=1):\n",
    "        if country_index < 1:\n",
    "            raise Exception(\"Exception: Country index must be greater than 0\")\n",
    "        \n",
    "        if not isinstance(self.table_data, pd.DataFrame):\n",
    "            tr_list = self.wiki_table.find_all(\"tr\")\n",
    "            table_contents = list(map(self.__extract_row_data, tr_list, [country_index] * len(tr_list)))\n",
    "            self.table_data = pd.DataFrame(\n",
    "                table_contents[1:], \n",
    "                columns=table_contents[0],\n",
    "                index=[row[country_index - 1] for row in table_contents[1:]] \n",
    "            )\n",
    "            self.table_data.drop(\n",
    "                columns=self.table_data.columns[:country_index],\n",
    "                axis=1,\n",
    "                inplace=True\n",
    "            )\n",
    "\n",
    "        if get_data:\n",
    "            return self.get_table()\n",
    "\n",
    "    # update table, usually after some prelim data cleaning is done\n",
    "    def update_table(self, new_table):\n",
    "        if isinstance(new_table, pd.DataFrame):\n",
    "            self.table_data = new_table\n",
    "        else:\n",
    "            raise Exception(\"Exception: The updated table must be of type pd.DataFrame.\")\n",
    "\n",
    "    # save data in xlsx file\n",
    "    def export_table(self):\n",
    "        if isinstance(self.table_data, pd.DataFrame):\n",
    "            file_name = self.wiki_category + \"__\" + self.wiki_title.replace(\" \", \"_\")  + \".xlsx\"\n",
    "            self.table_data.to_excel(file_name, index=True)\n",
    "        else:\n",
    "            raise Exception(\"Exception: Table contains no data\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d4b54-115e-4875-a44a-bb26b8361d0f",
   "metadata": {},
   "source": [
    "## Example use-case\n",
    "#### Percent of population living on less than \\\\$1.90, \\\\$3.20 and \\\\$5.50 a day\r\n",
    "Source: https://en.wikipedia.org/wiki/List_of_sovereign_states_by_percentage_of_population_living_in_povert\n",
    "<br>International (PPP) dollars as per the World Bank, the World Poverty Clock, and the Our World in Data.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0dfdad-2600-40e4-a434-979ed6a4f31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 166 rows in this data.\n",
      "There are 3 columns in this data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt; $1.90[1][5]</th>\n",
       "      <th>&lt; $3.20[6]</th>\n",
       "      <th>&lt; $5.50[7]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.3840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.2083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.7279</td>\n",
       "      <td>0.8913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Argentina</th>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.1820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Armenia</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.4470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           < $1.90[1][5]  < $3.20[6]  < $5.50[7]\n",
       "Albania           0.1000      0.2660      0.3840\n",
       "Algeria           0.0032      0.0223      0.2083\n",
       "Angola            0.5140      0.7279      0.8913\n",
       "Argentina         0.0160      0.0580      0.1820\n",
       "Armenia           0.0040      0.0690      0.4470"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create WikiTable object from wikipage\n",
    "page_url = \"https://en.wikipedia.org/wiki/List_of_sovereign_states_by_percentage_of_population_living_in_poverty\"\n",
    "category = \"Demographics\"\n",
    "table_num = 2\n",
    "wiki_page = WikiTable(*[page_url, category, table_num])\n",
    "\n",
    "# Preliminary cleaning\n",
    "wiki_page.set_notes(\"N/A\")\n",
    "table = wiki_page.create_table(get_data=True, country_index=1)\n",
    "table.drop(\n",
    "    columns=table.columns[-2:],\n",
    "    axis=1,\n",
    "    inplace=True\n",
    ")\n",
    "table.replace(to_replace=[\"%\"], value=\"\", inplace=True, regex=True)\n",
    "table = table.astype(float, copy=True) / 100\n",
    "\n",
    "# Save data\n",
    "wiki_page.update_table(table)\n",
    "#wiki_page.export_table()\n",
    "\n",
    "print(f\"There are {table.shape[0]} rows in this data.\")\n",
    "print(f\"There are {table.shape[1]} columns in this data.\")\n",
    "table.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
