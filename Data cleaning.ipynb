{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0fdf9a-e5e1-4929-89cf-597bdcf53fe0",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "In this notebook we seek to aggregate the data we've scraped and gathered, clean it, and prepare it for use in our modelling (whether primarily statistical or ML-based). Data has been extracted from multiple sources (primarily wikipedia tables), saved in excel flat files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23df8b6f-efe2-4d37-b550-db38d546bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c96b3e-3f02-42e0-9deb-38fd0755fcee",
   "metadata": {},
   "source": [
    "### Load Raw Data\n",
    "Here we load raw data from excel files in the designated directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b6d2549-01bf-4670-b896-d157545ccd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve master list of destinations\n",
    "destinations = pd.read_excel(\n",
    "    io=\"Travel_Destinations.xlsx\",\n",
    "    header=None\n",
    ")[0].values.tolist()\n",
    "\n",
    "# Retrieve file names and contruct paths for all raw data files\n",
    "dir_name = \"raw_data\"\n",
    "data_files = os.listdir(dir_name)\n",
    "data_paths = list()\n",
    "for file_name in data_files:\n",
    "    data_paths.append((file_name, os.path.join(dir_name, file_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fd64b-8757-4acb-9e9a-b1b1bbecce45",
   "metadata": {},
   "source": [
    "### Fuzzy matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5270c3d-08ec-4989-af4d-50114671e388",
   "metadata": {},
   "source": [
    "The goal here is that for each table in an excel flat file, given an instance in the data which has an associated destination, we want to compare the destination in the data with our master list of destinations and use fuzzy matching to select the most appropriate match. We replace the original destination with the matched destination. Upon completion, each data set will have the same set of destinations listed, making aggregation more accurate and feasible.\n",
    "\n",
    "Fuzzy matching typically uses the Levenshtein distance (aka the edit distance) or Indel distance as a basis. Both compute the number of operations, or edits, required to transform one string into another. The deletion, insertion, or substitution of a character all result in +1 to the Leneshtein distance (all actions are weighted equally). Related, the Indel distance is similar to the Levenshtein distance except it does not permit substitutions. Effectively, any substitution that would have incurred a cost of +1 is replaced by an insertion and deletion with cost +2, or equivalently, a substitution is given a weight of +2. The normalized distance for Levenshtein is computed as $$\\frac{distance}{max(len(string1),\\ len(string2)}$$ whereas the normalized distance for Indel is computed as $$\\frac{distance}{len(string1)\\ +\\ len(string2)}$$\n",
    "\n",
    "The similarity between two strings is $1-distance$. When using a specific ratio in the FuzzyWuzzy package, the function preprocesses the input strings in some way and then computes the scaled Levenshtein similarity. The Token Sort Ratio splits a string into its tokens/words, makes them lowercase and removes punctuations, then sorts the tokens alphabetically and joins them prior to computing the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62150279-55d1-4575-8b98-f67d6a1f5b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1 did not match 0 entries\n",
      "Table 2 did not match 0 entries\n",
      "Table 3 did not match 0 entries\n",
      "Table 4 did not match 0 entries\n",
      "Table 5 did not match 0 entries\n",
      "Table 6 did not match 0 entries\n",
      "Table 7 did not match 0 entries\n",
      "Table 8 did not match 0 entries\n",
      "Table 9 did not match 0 entries\n",
      "Table 10 did not match 0 entries\n",
      "Table 11 did not match 0 entries\n",
      "Table 12 did not match 0 entries\n",
      "Table 13 did not match 0 entries\n",
      "Table 14 did not match 0 entries\n",
      "Table 15 did not match 0 entries\n",
      "Table 16 did not match 0 entries\n",
      "Table 17 did not match 0 entries\n",
      "Table 18 did not match 0 entries\n",
      "Table 19 did not match 0 entries\n",
      "Table 20 did not match 0 entries\n",
      "Table 21 did not match 0 entries\n",
      "Table 22 did not match 0 entries\n",
      "Table 23 did not match 0 entries\n",
      "Table 24 did not match 0 entries\n",
      "Table 25 did not match 0 entries\n",
      "Table 26 did not match 0 entries\n"
     ]
    }
   ],
   "source": [
    "fuzzy_dataframes = list()\n",
    "fuzzy_path = \"fuzzy_data\"\n",
    "if not os.path.exists(fuzzy_path):\n",
    "    os.mkdir(fuzzy_path)\n",
    "\n",
    "# for every file and table within\n",
    "table_count = 0\n",
    "for data_path in data_paths:\n",
    "    file_name = data_path[0]\n",
    "    file_path = data_path[1]\n",
    "    table_count += 1\n",
    "    \n",
    "    # open the file\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # read the file and extract its list of destinations\n",
    "        unmatched_data = pd.read_excel(f, index_col=0)\n",
    "        unmatched_destinations = unmatched_data.index.values.tolist()\n",
    "        \n",
    "        # perform fuzzy match \n",
    "        matched_destinations = list(map(\n",
    "            process.extractOne,  # function to be applied\n",
    "            unmatched_destinations,  # applied to \n",
    "            repeat(destinations),  # compared against\n",
    "            repeat(process.default_processor),  # use default processor each time\n",
    "            repeat(fuzz.token_sort_ratio),  # use token sort ratio instead of default\n",
    "            repeat(75)  # similarity must be higher than 50% to be considered a match\n",
    "        ))\n",
    "\n",
    "        # debugging the fuzzy match logic\n",
    "        unmatched_count = matched_destinations.count(None)\n",
    "        print(f\"Table {table_count} did not match {unmatched_count} entries\")\n",
    "        if unmatched_count > 0:\n",
    "            unmatched_indices = [i for i, match in enumerate(matched_destinations) if match == None]\n",
    "        \n",
    "        # check if any entries are None indicating no match, if so, set to previous value \n",
    "        matched_destinations = [unmatched_destinations[index] if match == None else match[0] for index, match in enumerate(matched_destinations)]\n",
    "\n",
    "        # debugging the fuzzy match logic\n",
    "        if unmatched_count > 0:\n",
    "            for i in range(unmatched_count):\n",
    "                print(f\"---{matched_destinations[unmatched_indices[i]]}\")\n",
    "                destinations.append(matched_destinations[unmatched_indices[i]])  # add legitimate unmatched destinations to main list\n",
    "        \n",
    "        # reindex the data to the fuzzy matched destinations\n",
    "        matched_data = unmatched_data.set_index(pd.Index(matched_destinations))\n",
    "\n",
    "        # save the file name in the fuzzy folder\n",
    "        matched_data.to_excel(os.path.join(fuzzy_path, file_name))\n",
    "        fuzzy_dataframes.append(matched_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28b22c72-bc5c-4200-981a-0650c2a855d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer join all data based on the destinations\n",
    "joined_data = fuzzy_dataframes[2].join(\n",
    "    fuzzy_dataframes[0:2]+fuzzy_dataframes[3:],\n",
    "    how=\"outer\",\n",
    "    sort=True\n",
    ")\n",
    "\n",
    "# remove duplicate rows that end up in the data\n",
    "joined_data = joined_data[~joined_data.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394c425-8508-487a-9e8f-553a85c0bbcd",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae5961-5478-4a4d-9f98-e9dfca74d681",
   "metadata": {},
   "source": [
    "Here we perform imputation to fill in missing values. There are no destinations in this data that have all missing values. We do not drop any destinations from the analysis as we prefer to have a broader set to recommend to end-users.\n",
    "\n",
    "In this analysis we use multiple imputation by chained equations (MICE) as our imputation method, due to its attempts to account for the uncertainty in the imputed value by provingd both within-imputation and between-imputation variability, a characteristic which makes multiple imputation preferable over single imputation in most circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6a53d-dd77-4cdb-9182-418e12899091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "len(joined_data[joined_data.count(axis=1) <= joined_data.shape[1]*.35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d2d1a62f-7be2-40f2-8454-1d6793bc3ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ed4c834-0584-4157-a030-dfa1aa858eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22090675-a43f-415e-8b19-6293b968fe99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
